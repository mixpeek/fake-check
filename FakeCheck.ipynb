{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2qqxWyGif1s",
        "outputId": "5f929663-196a-4afa-9e97-a7c6ae54cd3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 0: Upgrading pip ---\n",
            "\n",
            "--- Step 1: Removing conflicting Colab defaults ---\n",
            "\n",
            "--- Step 2: Installing critical base packages ---\n",
            "\n",
            "--- Step 2.5: Installing stable google-api-core ---\n",
            "\n",
            "--- Verification 1: Base packages ---\n",
            "NumPy version: 1.26.4\n",
            "Pillow version: 9.5.0\n",
            "Protobuf version: 4.25.8\n",
            "google-api-core version: 2.11.1\n",
            "✅ PIL.ImageFont imported successfully\n",
            "\n",
            "--- Step 3: Installing PyTorch stack ---\n",
            "\n",
            "--- Step 4: Installing Transformers stack ---\n",
            "\n",
            "--- Step 5: Installing ftfy and open-clip ---\n",
            "\n",
            "--- Verification 2: Torch and vision stack ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PyTorch version: 2.2.1+cu118\n",
            "✅ open-clip imported successfully\n",
            "\n",
            "--- Step 6: Installing Google Generative AI ---\n",
            "\n",
            "--- Step 7: Pre-installing Whisper dependencies ---\n",
            "\n",
            "--- Step 8: Installing OpenAI Whisper ---\n",
            "\n",
            "--- Step 9: Installing other utilities ---\n",
            "\n",
            "--- Step 10: Final version enforcement ---\n",
            "\n",
            "--- FINAL VERIFICATION ---\n",
            "✅ PIL: 9.5.0 (expected: 9.5.0)\n",
            "    ✅ PIL.ImageFont works with Pillow 9.5.0\n",
            "✅ numpy: 1.26.4 (expected: 1.26.4)\n",
            "✅ google.api_core: 2.11.1 (expected: 2.11.1)\n",
            "✅ torch: 2.2.1+cu118 (expected: 2.2.1+cu118)\n",
            "✅ transformers: 4.40.2\n",
            "✅ huggingface_hub: 0.33.0\n",
            "✅ open_clip: N/A\n",
            "✅ whisper: 20231117 (expected: 20231117)\n",
            "✅ cv2: 4.9.0\n",
            "⚠️ google.generativeai: N/A (expected: 0.5.2)\n",
            "\n",
            "==================================================\n",
            "✅ Environment setup targeted critical versions. Check ⚠️ for non-critical or resolved versions.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 01: Robust Environment Setup with Force Installation\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "# 0. Function to force package installation with system override\n",
        "def force_install_package(package_spec):\n",
        "    \"\"\"Force installation ignoring system packages\"\"\"\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
        "                          '--force-reinstall', '--no-deps', package_spec])\n",
        "\n",
        "def install_with_deps(package_spec):\n",
        "    \"\"\"Install package with dependencies\"\"\"\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
        "                          '--force-reinstall', package_spec])\n",
        "\n",
        "# 1. Upgrade pip first\n",
        "print(\"--- Step 0: Upgrading pip ---\")\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'])\n",
        "\n",
        "# 2. CRITICAL: Uninstall Colab's default packages that conflict\n",
        "print(\"\\n--- Step 1: Removing conflicting Colab defaults ---\")\n",
        "# Uninstall packages that Colab pre-installs which conflict with our requirements\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y',\n",
        "               'Pillow', 'numpy', 'protobuf'], capture_output=True)\n",
        "\n",
        "# 3. Install core dependencies with explicit force\n",
        "print(\"\\n--- Step 2: Installing critical base packages ---\")\n",
        "# Install NumPy 1.26.4 first as many packages depend on it\n",
        "force_install_package('numpy==1.26.4')\n",
        "\n",
        "# Install Pillow 9.5.0 to fix the is_directory error\n",
        "force_install_package('Pillow==9.5.0')\n",
        "\n",
        "# Install protobuf early (use 4.x since MediaPipe is removed)\n",
        "force_install_package('protobuf==4.25.3')\n",
        "\n",
        "# *** NEW STEP: Install a stable google-api-core ***\n",
        "print(\"\\n--- Step 2.5: Installing stable google-api-core ---\")\n",
        "install_with_deps('google-api-core[grpc]~=2.11.1') # Or another stable 2.x like 2.15.0\n",
        "\n",
        "# 4. Verify critical packages before proceeding\n",
        "print(\"\\n--- Verification 1: Base packages ---\")\n",
        "importlib.invalidate_caches()\n",
        "import numpy\n",
        "import PIL\n",
        "import google.protobuf\n",
        "import google.api_core # Verify this new addition\n",
        "\n",
        "print(f\"NumPy version: {numpy.__version__}\")\n",
        "print(f\"Pillow version: {PIL.__version__}\")\n",
        "print(f\"Protobuf version: {google.protobuf.__version__}\")\n",
        "print(f\"google-api-core version: {google.api_core.__version__}\") # Check its version\n",
        "\n",
        "assert numpy.__version__ == \"1.26.4\", f\"NumPy version mismatch: {numpy.__version__}\"\n",
        "assert PIL.__version__ == \"9.5.0\", f\"Pillow version mismatch: {PIL.__version__}\"\n",
        "assert google.api_core.__version__.startswith(\"2.11.1\"), f\"google-api-core version mismatch: {google.api_core.__version__}\"\n",
        "\n",
        "# Test critical imports\n",
        "try:\n",
        "    from PIL import ImageFont\n",
        "    print(\"✅ PIL.ImageFont imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ PIL.ImageFont import failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# 5. Install PyTorch with CUDA support\n",
        "print(\"\\n--- Step 3: Installing PyTorch stack ---\")\n",
        "torch_cmd = [sys.executable, '-m', 'pip', 'install',\n",
        "             'torch==2.2.1+cu118', 'torchvision==0.17.1+cu118',\n",
        "             'torchaudio==2.2.1+cu118',\n",
        "             '--index-url', 'https://download.pytorch.org/whl/cu118']\n",
        "subprocess.check_call(torch_cmd)\n",
        "\n",
        "# 6. Install transformers ecosystem with specific versions\n",
        "print(\"\\n--- Step 4: Installing Transformers stack ---\")\n",
        "# Quote version constraints to avoid shell interpretation\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', \"transformers>=4.30.0,<4.41.0\"])\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', \"huggingface-hub>=0.20.0\"])\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', \"tokenizers>=0.14.0\"])\n",
        "\n",
        "# 7. Install ftfy before open-clip\n",
        "print(\"\\n--- Step 5: Installing ftfy and open-clip ---\")\n",
        "install_with_deps('ftfy>=6.0')\n",
        "force_install_package('open-clip-torch==2.23.0')\n",
        "\n",
        "# 8. Verify torch and open-clip\n",
        "print(\"\\n--- Verification 2: Torch and vision stack ---\")\n",
        "importlib.invalidate_caches()\n",
        "try:\n",
        "    import torch\n",
        "    import open_clip\n",
        "    print(f\"✅ PyTorch version: {torch.__version__}\")\n",
        "    print(\"✅ open-clip imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "# 9. Install Google Cloud Libraries\n",
        "print(\"\\n--- Step 6: Installing Google Generative AI ---\")\n",
        "install_with_deps('google-generativeai==0.5.2')\n",
        "# install_with_deps('google-cloud-vision~=3.4')\n",
        "\n",
        "# 10. Pre-install compatible versions for Whisper dependencies\n",
        "print(\"\\n--- Step 7: Pre-installing Whisper dependencies ---\")\n",
        "# Install numba compatible with numpy 1.26.4\n",
        "install_with_deps('numba==0.58.1')\n",
        "\n",
        "# Install spacy 3.4.4 to get thinc 8.1.x (compatible with numpy 1.26.4)\n",
        "# This prevents whisper from pulling thinc 8.3.6 which requires numpy 2.x\n",
        "install_with_deps('spacy==3.4.4')\n",
        "install_with_deps('thinc>=8.1.0,<8.2.0')\n",
        "\n",
        "# 11. Install Whisper\n",
        "print(\"\\n--- Step 8: Installing OpenAI Whisper ---\")\n",
        "install_with_deps('openai-whisper==20231117')\n",
        "\n",
        "# 12. Install remaining utilities\n",
        "print(\"\\n--- Step 9: Installing other utilities ---\")\n",
        "install_with_deps('ffmpeg-python==0.2.0')\n",
        "install_with_deps('opencv-python-headless==4.9.0.80')\n",
        "install_with_deps('nest-asyncio==1.6.0')\n",
        "\n",
        "# 13. Force reinstall our exact versions one more time to ensure they stick\n",
        "print(\"\\n--- Step 10: Final version enforcement ---\")\n",
        "force_install_package('numpy==1.26.4')\n",
        "force_install_package('Pillow==9.5.0')\n",
        "\n",
        "# 14. Final comprehensive verification\n",
        "print(\"\\n--- FINAL VERIFICATION ---\")\n",
        "importlib.invalidate_caches()\n",
        "\n",
        "overall_setup_ok_final = True\n",
        "\n",
        "def verify_import(module_name, version_attr='__version__', expected_version=None, critical=False):\n",
        "    global overall_setup_ok_final # Use the renamed global\n",
        "    try:\n",
        "        module = importlib.import_module(module_name)\n",
        "        version = getattr(module, version_attr, 'N/A')\n",
        "        status = \"✅\"\n",
        "        message = f\"{module_name}: {version}\"\n",
        "        if expected_version:\n",
        "            message += f\" (expected: {expected_version})\"\n",
        "            if version != expected_version:\n",
        "                status = \"⚠️\"\n",
        "                if critical: overall_setup_ok_final = False # Fail build on critical mismatch\n",
        "        print(f\"{status} {message}\")\n",
        "\n",
        "        if module_name == \"PIL\" and expected_version == \"9.5.0\" and version == \"9.5.0\":\n",
        "             from PIL import ImageFont # Test only if Pillow is our target version\n",
        "             print(\"    ✅ PIL.ImageFont works with Pillow 9.5.0\")\n",
        "        return True\n",
        "    except ImportError as e:\n",
        "        print(f\"❌ {module_name}: Import failed - {e}\")\n",
        "        if critical: overall_setup_ok_final = False\n",
        "        return False\n",
        "    except Exception as e_gen:\n",
        "        print(f\"❌ {module_name}: Verification error - {e_gen}\")\n",
        "        if critical: overall_setup_ok_final = False\n",
        "        return False\n",
        "\n",
        "# Critical version checks\n",
        "verify_import('PIL', expected_version='9.5.0', critical=True)\n",
        "verify_import('numpy', expected_version='1.26.4', critical=True)\n",
        "verify_import('google.api_core', expected_version='2.11.1', critical=True) # Verify pinned GAC\n",
        "\n",
        "# Other important checks\n",
        "verify_import('torch', expected_version='2.2.1+cu118')\n",
        "verify_import('transformers') # No strict version, just check import\n",
        "verify_import('huggingface_hub') # No strict version\n",
        "verify_import('open_clip')\n",
        "verify_import('whisper', expected_version='20231117') # package version\n",
        "verify_import('cv2')\n",
        "verify_import('google.generativeai', version_attr='VERSION', expected_version='0.5.2')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "if overall_setup_ok_final:\n",
        "    print(\"✅ Environment setup targeted critical versions. Check ⚠️ for non-critical or resolved versions.\")\n",
        "else:\n",
        "    print(\"❌ Critical issues remain in environment setup. Check errors above.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VApthe7mZPd",
        "outputId": "20c8171d-385c-44b7-8921-e18bc32ab81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 BACKEND MODULE SETUP\n",
            "============================================================\n",
            "\n",
            "🚨 CRITICAL: You MUST upload 5 backend files before proceeding!\n",
            "\n",
            "📋 REQUIRED FILES from backend/app/core/ directory:\n",
            "\n",
            "1. 📄 flow.py      - Optical flow spike detection\n",
            "2. 📄 fusion.py    - Score fusion and thresholds  \n",
            "3. 📄 gemini.py    - Gemini API interactions (parallel)\n",
            "4. 📄 models.py    - CLIP scoring & Whisper transcription\n",
            "5. 📄 video.py     - Video sampling functions\n",
            "\n",
            "HOW TO UPLOAD:\n",
            "1. Click 'Files' tab in left sidebar\n",
            "2. Click 'Upload to session storage' \n",
            "3. Select all 5 .py files from your local backend/app/core/ folder\n",
            "4. Wait for upload to complete\n",
            "5. Re-run this cell\n",
            "\n",
            "⚠️  IMPORTANT: These must be the exact files from the repository!\n",
            "     Do NOT create empty files or copy code manually!\n",
            "\n",
            "\n",
            "📦 Backend Module Status:\n",
            "   flow.py: ✅ Loaded\n",
            "   fusion.py: ✅ Loaded\n",
            "   gemini.py: ✅ Loaded\n",
            "   models.py: ✅ Loaded\n",
            "   video.py: ✅ Loaded\n",
            "\n",
            "🔍 Debug: Files in /content directory:\n",
            "   📄 fusion.py (4.9 KB)\n",
            "   📄 models.py (5.9 KB)\n",
            "   📄 video.py (13.3 KB)\n",
            "   📄 gemini.py (38.1 KB)\n",
            "   📄 flow.py (2.4 KB)\n",
            "\n",
            "🔍 Debug: Functions in loaded 'video' module:\n",
            "   • Any\n",
            "   • Dict\n",
            "   • List\n",
            "   • Optional\n",
            "   • Tuple\n",
            "   • detect_lighting_jumps\n",
            "   • extract_audio\n",
            "   • sample_video_content\n",
            "\n",
            "🔧 Configuration:\n",
            "   Device: cuda\n",
            "   CLIP Model: ViT-L-14\n",
            "   Whisper Model: base.en\n",
            "   Target FPS: 8\n",
            "\n",
            "🔐 Google Cloud Authentication:\n",
            "✅ Google Cloud authentication successful\n",
            "✅ Gemini API configured\n",
            "\n",
            "🤖 Loading Models...\n",
            "✅ CLIP Model loaded: ViT-L-14\n",
            "✅ Whisper Model loaded: base.en\n",
            "✅ Gemini Model initialized: gemini-2.5-pro-preview-05-06\n",
            "\n",
            "📊 Setup Status:\n",
            "   Backend Modules: 5/5 loaded\n",
            "   Essential Models: ✅ Ready\n",
            "   Gemini Available: ✅ Yes\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Imports, Backend Module Loading & Model Setup\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import uuid\n",
        "import asyncio\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Optional, Tuple\n",
        "from datetime import datetime\n",
        "from IPython.display import display, Markdown\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio for running asyncio code in Colab cells\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Add /content to Python path for backend modules\n",
        "if '/content' not in sys.path:\n",
        "    sys.path.append('/content')\n",
        "\n",
        "print(\"📁 BACKEND MODULE SETUP\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "🚨 CRITICAL: You MUST upload 5 backend files before proceeding!\n",
        "\n",
        "📋 REQUIRED FILES from backend/app/core/ directory:\n",
        "\n",
        "1. 📄 flow.py      - Optical flow spike detection\n",
        "2. 📄 fusion.py    - Score fusion and thresholds\n",
        "3. 📄 gemini.py    - Gemini API interactions (parallel)\n",
        "4. 📄 models.py    - CLIP scoring & Whisper transcription\n",
        "5. 📄 video.py     - Video sampling functions\n",
        "\n",
        "HOW TO UPLOAD:\n",
        "1. Click 'Files' tab in left sidebar\n",
        "2. Click 'Upload to session storage'\n",
        "3. Select all 5 .py files from your local backend/app/core/ folder\n",
        "4. Wait for upload to complete\n",
        "5. Re-run this cell\n",
        "\n",
        "⚠️  IMPORTANT: These must be the exact files from the repository!\n",
        "     Do NOT create empty files or copy code manually!\n",
        "\"\"\")\n",
        "\n",
        "# Import backend modules with graceful degradation\n",
        "modules_status = {}\n",
        "backend_modules = ['flow', 'fusion', 'gemini', 'models', 'video']\n",
        "\n",
        "for module_name in backend_modules:\n",
        "    try:\n",
        "        # First check if file exists in /content\n",
        "        module_path = f'/content/{module_name}.py'\n",
        "        if not os.path.exists(module_path):\n",
        "            modules_status[module_name] = f'❌ Missing: File not found at {module_path}'\n",
        "            print(f\"⚠️ {module_name}.py not found in /content/ - some features will be disabled\")\n",
        "            continue\n",
        "\n",
        "        # Import the module from /content specifically\n",
        "        import importlib.util\n",
        "        spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
        "        if spec and spec.loader:\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            spec.loader.exec_module(module)\n",
        "\n",
        "            # Verify the module has expected functions\n",
        "            expected_functions = {\n",
        "                'video': ['sample_video_content'],\n",
        "                'models': ['calculate_visual_clip_score', 'transcribe_audio_content'],\n",
        "                'gemini': ['run_gemini_inspections'],\n",
        "                'flow': ['detect_spikes'],\n",
        "                'fusion': ['fuse_detection_scores']\n",
        "            }\n",
        "\n",
        "            if module_name in expected_functions:\n",
        "                missing_functions = []\n",
        "                for func_name in expected_functions[module_name]:\n",
        "                    if not hasattr(module, func_name):\n",
        "                        missing_functions.append(func_name)\n",
        "\n",
        "                if missing_functions:\n",
        "                    modules_status[module_name] = f'❌ Missing functions: {\", \".join(missing_functions)}'\n",
        "                    print(f\"⚠️ {module_name}.py missing functions: {missing_functions}\")\n",
        "                    continue\n",
        "\n",
        "            modules_status[module_name] = '✅ Loaded'\n",
        "            globals()[module_name] = module\n",
        "            sys.modules[module_name] = module  # Also add to sys.modules\n",
        "        else:\n",
        "            modules_status[module_name] = f'❌ Import error: Could not create module spec'\n",
        "            print(f\"⚠️ Failed to create import spec for {module_name}.py\")\n",
        "\n",
        "    except Exception as e:\n",
        "        modules_status[module_name] = f'❌ Import error: {e}'\n",
        "        print(f\"⚠️ Error importing {module_name}.py: {e}\")\n",
        "\n",
        "print(\"\\n📦 Backend Module Status:\")\n",
        "for module, status in modules_status.items():\n",
        "    print(f\"   {module}.py: {status}\")\n",
        "\n",
        "# Debug: Show what files are actually in /content\n",
        "print(\"\\n🔍 Debug: Files in /content directory:\")\n",
        "try:\n",
        "    content_files = [f for f in os.listdir('/content') if f.endswith('.py')]\n",
        "    if content_files:\n",
        "        for f in content_files:\n",
        "            file_path = f'/content/{f}'\n",
        "            size_kb = os.path.getsize(file_path) / 1024\n",
        "            print(f\"   📄 {f} ({size_kb:.1f} KB)\")\n",
        "    else:\n",
        "        print(\"   ❌ No .py files found in /content/\")\n",
        "        print(\"\\n💡 UPLOAD REQUIRED:\")\n",
        "        print(\"   1. Click 'Files' tab in left sidebar\")\n",
        "        print(\"   2. Click 'Upload to session storage'\")\n",
        "        print(\"   3. Upload these 5 files: flow.py, fusion.py, gemini.py, models.py, video.py\")\n",
        "        print(\"   4. Re-run this cell\")\n",
        "except Exception as e:\n",
        "    print(f\"   ❌ Error checking /content: {e}\")\n",
        "\n",
        "# Debug: Show what functions are available in the video module if it was loaded\n",
        "if 'video' in globals():\n",
        "    print(f\"\\n🔍 Debug: Functions in loaded 'video' module:\")\n",
        "    video_functions = [attr for attr in dir(video) if not attr.startswith('_') and callable(getattr(video, attr))]\n",
        "    if video_functions:\n",
        "        for func in video_functions:\n",
        "            print(f\"   • {func}\")\n",
        "    else:\n",
        "        print(\"   ❌ No functions found in video module\")\n",
        "else:\n",
        "    print(f\"\\n🔍 Debug: 'video' module not loaded in globals()\")\n",
        "\n",
        "# Essential ML imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import PIL.Image as Image\n",
        "import open_clip\n",
        "import whisper\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configuration (matching backend)\n",
        "CLIP_MODEL_NAME = \"ViT-L-14\"\n",
        "CLIP_PRETRAINED = \"laion2b_s32b_b82k\"\n",
        "WHISPER_MODEL_NAME = \"base.en\"\n",
        "GEMINI_MODEL_NAME = \"gemini-2.5-pro-preview-05-06\"\n",
        "TARGET_FPS = 8\n",
        "MAX_VIDEO_DURATION_SEC = 30\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"\\n🔧 Configuration:\")\n",
        "print(f\"   Device: {DEVICE}\")\n",
        "print(f\"   CLIP Model: {CLIP_MODEL_NAME}\")\n",
        "print(f\"   Whisper Model: {WHISPER_MODEL_NAME}\")\n",
        "print(f\"   Target FPS: {TARGET_FPS}\")\n",
        "\n",
        "# Google Cloud Authentication (Colab specific)\n",
        "print(\"\\n🔐 Google Cloud Authentication:\")\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    print(\"✅ Google Cloud authentication successful\")\n",
        "except ImportError:\n",
        "    print(\"⚠️ Not running in Google Colab\")\n",
        "\n",
        "# API Key Setup\n",
        "GEMINI_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    if GEMINI_API_KEY:\n",
        "        genai.configure(api_key=GEMINI_API_KEY)\n",
        "        print(\"✅ Gemini API configured\")\n",
        "    else:\n",
        "        print(\"⚠️ GEMINI_API_KEY not found in Colab Secrets\")\n",
        "except ImportError:\n",
        "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "    if GEMINI_API_KEY:\n",
        "        genai.configure(api_key=GEMINI_API_KEY)\n",
        "        print(\"✅ Gemini API configured from environment\")\n",
        "    else:\n",
        "        print(\"⚠️ GEMINI_API_KEY not found\")\n",
        "\n",
        "# Initialize Models\n",
        "print(\"\\n🤖 Loading Models...\")\n",
        "CLIP_MODEL, CLIP_PREPROCESS_FN = None, None\n",
        "WHISPER_MODEL = None\n",
        "GEMINI_MODEL = None\n",
        "\n",
        "# CLIP Model\n",
        "if 'models' in modules_status and '✅' in modules_status['models']:\n",
        "    try:\n",
        "        CLIP_MODEL, _, CLIP_PREPROCESS_FN = open_clip.create_model_and_transforms(\n",
        "            CLIP_MODEL_NAME, pretrained=CLIP_PRETRAINED, device=DEVICE\n",
        "        )\n",
        "        CLIP_MODEL.eval()\n",
        "        print(f\"✅ CLIP Model loaded: {CLIP_MODEL_NAME}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CLIP Model error: {e}\")\n",
        "\n",
        "# Whisper Model\n",
        "try:\n",
        "    WHISPER_MODEL = whisper.load_model(WHISPER_MODEL_NAME, device=DEVICE)\n",
        "    print(f\"✅ Whisper Model loaded: {WHISPER_MODEL_NAME}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Whisper Model error: {e}\")\n",
        "\n",
        "# Gemini Model\n",
        "if GEMINI_API_KEY and 'gemini' in modules_status and '✅' in modules_status['gemini']:\n",
        "    try:\n",
        "        GEMINI_MODEL = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
        "        print(f\"✅ Gemini Model initialized: {GEMINI_MODEL_NAME}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Gemini Model error: {e}\")\n",
        "\n",
        "# Status Summary\n",
        "essential_loaded = all([CLIP_MODEL, WHISPER_MODEL])\n",
        "print(f\"\\n📊 Setup Status:\")\n",
        "print(f\"   Backend Modules: {sum('✅' in status for status in modules_status.values())}/5 loaded\")\n",
        "print(f\"   Essential Models: {'✅ Ready' if essential_loaded else '❌ Issues'}\")\n",
        "print(f\"   Gemini Available: {'✅ Yes' if GEMINI_MODEL else '⚠️ Disabled'}\")\n",
        "\n",
        "if not essential_loaded:\n",
        "    print(\"\\n⚠️ Some components missing - pipeline will run with reduced functionality\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "fJnFXJSSmj63"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Main Detection Pipeline (Backend Integration)\n",
        "\n",
        "async def run_detection_pipeline(video_path: str, job_id: str = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Main deepfake detection pipeline matching backend implementation.\n",
        "    Returns the same JSON structure as the API.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    video_basename = os.path.basename(video_path)\n",
        "    run_id = f\"{Path(video_basename).stem}_{(job_id or uuid.uuid4().hex)[:6]}\"\n",
        "\n",
        "    result = {\n",
        "        \"input_video\": video_basename,\n",
        "        \"run_id\": run_id,\n",
        "        \"pipeline_version\": \"notebook_backend_integration_v1\"\n",
        "    }\n",
        "\n",
        "    temp_audio_path = None\n",
        "\n",
        "    try:\n",
        "        print(f\"🔍 Processing: {video_basename}\")\n",
        "\n",
        "        # Step 1: Sample video content\n",
        "        print(\"📹 Step 1: Sampling video content...\")\n",
        "\n",
        "        if 'video' not in globals():\n",
        "            raise RuntimeError(\"video.py module not loaded - please upload video.py to /content/\")\n",
        "\n",
        "        # Get video module from globals to avoid UnboundLocalError\n",
        "        video_module = globals()['video']\n",
        "\n",
        "        # Try alternative access methods if hasattr fails\n",
        "        if not hasattr(video_module, 'sample_video_content'):\n",
        "            print(\"🔧 Video function not found, trying direct import...\")\n",
        "\n",
        "            # Try re-importing directly\n",
        "            try:\n",
        "                import importlib.util\n",
        "                spec = importlib.util.spec_from_file_location(\"video_direct\", \"/content/video.py\")\n",
        "                video_direct = importlib.util.module_from_spec(spec)\n",
        "                spec.loader.exec_module(video_direct)\n",
        "                sample_func_direct = getattr(video_direct, 'sample_video_content', None)\n",
        "                if sample_func_direct:\n",
        "                    print(\"✅ Using directly imported video module\")\n",
        "                    video_module = video_direct  # Replace the problematic module\n",
        "                    globals()['video'] = video_direct  # Update global reference\n",
        "                else:\n",
        "                    raise RuntimeError(\"sample_video_content not found in direct import\")\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"video.sample_video_content function not available: {e}\")\n",
        "\n",
        "        frames, temp_audio_path, original_dur, processed_dur = video_module.sample_video_content(\n",
        "            video_path,\n",
        "            target_fps=TARGET_FPS,\n",
        "            max_duration_sec=MAX_VIDEO_DURATION_SEC\n",
        "        )\n",
        "\n",
        "        result.update({\n",
        "            \"video_original_duration_sec\": round(original_dur, 2),\n",
        "            \"video_processed_duration_sec\": round(processed_dur, 2),\n",
        "            \"num_frames_sampled\": len(frames)\n",
        "        })\n",
        "\n",
        "        if not frames:\n",
        "            raise RuntimeError(\"Frame sampling returned no frames\")\n",
        "\n",
        "        # Step 2: CLIP visual score\n",
        "        print(\"🎨 Step 2: Calculating CLIP visual score...\")\n",
        "        clip_score = 0.0\n",
        "        if CLIP_MODEL and CLIP_PREPROCESS_FN and 'models' in globals():\n",
        "            clip_score = models.calculate_visual_clip_score(\n",
        "                frames, CLIP_MODEL, CLIP_PREPROCESS_FN, DEVICE\n",
        "            )\n",
        "        result[\"score_visual_clip\"] = round(clip_score, 3)\n",
        "\n",
        "        # Step 3: Whisper transcription\n",
        "        print(\"🎤 Step 3: Transcribing audio with Whisper...\")\n",
        "        transcription = {\"text\": \"\", \"words\": [], \"avg_no_speech_prob\": 1.0, \"language\": \"unknown\"}\n",
        "        if WHISPER_MODEL and temp_audio_path and 'models' in globals():\n",
        "            transcription = models.transcribe_audio_content(temp_audio_path, WHISPER_MODEL)\n",
        "\n",
        "        # Check for valid English speech\n",
        "        avg_no_speech_prob = transcription.get(\"avg_no_speech_prob\", 0.0)\n",
        "        detected_lang = transcription.get(\"language\", \"unknown\")\n",
        "        lipsync_enabled = True\n",
        "\n",
        "        result[\"detected_language\"] = detected_lang\n",
        "\n",
        "        if avg_no_speech_prob > 0.85:\n",
        "            print(f\"⚠️ High 'no speech' probability ({avg_no_speech_prob:.2f}) detected. Disabling lip-sync check.\")\n",
        "            lipsync_enabled = False\n",
        "            transcription[\"text\"] = \"[No speech detected]\"\n",
        "        elif detected_lang != 'en':\n",
        "            print(f\"⚠️ Detected language is '{detected_lang}', not 'en'. Disabling lip-sync check.\")\n",
        "            lipsync_enabled = False\n",
        "            transcription[\"text\"] = f\"[Non-English language detected: {detected_lang}]\"\n",
        "\n",
        "        result[\"transcript_snippet\"] = (\n",
        "            transcription[\"text\"][:150] + \"...\" if transcription[\"text\"] else \"[No Speech/Audio Error]\"\n",
        "        )\n",
        "\n",
        "        # Step 4: Gemini inspections\n",
        "        print(\"🔮 Step 4: Running Gemini inspections...\")\n",
        "        vis_flag = lip_flag = blink_flag = 0\n",
        "        gibberish_score = 0.0\n",
        "        gemini_events = []\n",
        "\n",
        "        if GEMINI_MODEL and 'gemini' in globals():\n",
        "            try:\n",
        "                print(f\"🔍 Debug: Gemini model type: {type(GEMINI_MODEL)}\")\n",
        "                print(f\"🔍 Debug: Number of frames: {len(frames)}\")\n",
        "                print(f\"🔍 Debug: Video duration: {processed_dur:.2f}s\")\n",
        "                print(f\"🔍 Debug: Lipsync enabled: {lipsync_enabled}\")\n",
        "\n",
        "                vis_flag, lip_flag, blink_flag, gibberish_score, gemini_events = await gemini.run_gemini_inspections(\n",
        "                    frames,\n",
        "                    video_path,\n",
        "                    transcription[\"text\"],\n",
        "                    GEMINI_MODEL,\n",
        "                    fps=TARGET_FPS,\n",
        "                    video_duration=processed_dur,\n",
        "                    job_id=job_id,\n",
        "                    enable_lipsync=lipsync_enabled\n",
        "                )\n",
        "\n",
        "                print(f\"🔍 Debug: Gemini results - vis:{vis_flag}, lip:{lip_flag}, blink:{blink_flag}, gibberish:{gibberish_score:.3f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Gemini inspections failed: {e}\")\n",
        "                import traceback\n",
        "                print(f\"🔍 Debug: Full traceback:\\n{traceback.format_exc()}\")\n",
        "        else:\n",
        "            if not GEMINI_MODEL:\n",
        "                print(\"⚠️ GEMINI_MODEL is None - Gemini checks disabled\")\n",
        "            if 'gemini' not in globals():\n",
        "                print(\"⚠️ gemini module not in globals - Gemini checks disabled\")\n",
        "\n",
        "        result.update({\n",
        "            \"flag_gemini_visual_artifact\": vis_flag,\n",
        "            \"flag_gemini_lipsync_issue\": lip_flag,\n",
        "            \"flag_gemini_abnormal_blinks\": blink_flag\n",
        "        })\n",
        "\n",
        "        # Step 5: Heuristic detectors\n",
        "        print(\"🔬 Step 5: Running heuristic detectors...\")\n",
        "        flow_result = {\"score\": 0.0, \"events\": [], \"tags\": []}\n",
        "\n",
        "        if 'flow' in globals():\n",
        "            try:\n",
        "                flow_result = flow.detect_spikes(frames, TARGET_FPS)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Flow detection failed: {e}\")\n",
        "\n",
        "        # Step 6: Score fusion\n",
        "        print(\"⚖️ Step 6: Fusing detection scores...\")\n",
        "\n",
        "        if 'fusion' in globals():\n",
        "            try:\n",
        "                other_scores = {\n",
        "                    \"gibberish\": gibberish_score,\n",
        "                    \"flow\": flow_result.get(\"score\", 0.0),\n",
        "                }\n",
        "\n",
        "                final_conf, final_label, fusion_tags, label_confidence = fusion.fuse_detection_scores(\n",
        "                    clip_score,\n",
        "                    vis_flag,\n",
        "                    lip_flag,\n",
        "                    blink_flag,\n",
        "                    other_scores=other_scores\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Score fusion failed: {e}\")\n",
        "                final_conf, final_label, fusion_tags, label_confidence = 0.5, \"UNCERTAIN\", [], 0.5\n",
        "        else:\n",
        "            # Simple fallback fusion\n",
        "            final_conf = (clip_score * 0.4 + (vis_flag + lip_flag + blink_flag) * 0.2)\n",
        "            final_label = \"LIKELY_FAKE\" if final_conf > 0.6 else \"LIKELY_REAL\" if final_conf < 0.3 else \"UNCERTAIN\"\n",
        "            fusion_tags = []\n",
        "            label_confidence = 0.5\n",
        "\n",
        "        result.update({\n",
        "            \"deepfake_confidence_overall\": final_conf,\n",
        "            \"label_confidence\": label_confidence,\n",
        "            \"final_predicted_label\": final_label,\n",
        "        })\n",
        "\n",
        "        # Aggregate anomaly tags and events\n",
        "        all_tags = list(fusion_tags)\n",
        "        all_tags.extend(flow_result.get(\"tags\", []))\n",
        "        if gibberish_score > 0:\n",
        "            all_tags.append(\"gibberish_text\")\n",
        "        result[\"anomaly_tags_detected\"] = sorted(list(set(all_tags)))\n",
        "\n",
        "        # Timeline events\n",
        "        timeline_events = []\n",
        "        timeline_events.extend(flow_result.get(\"events\", []))\n",
        "        timeline_events.extend(gemini_events)\n",
        "        timeline_events.sort(key=lambda ev: (ev.get(\"module\", \"\"), ev.get(\"ts\", 0.0)))\n",
        "        result[\"events\"] = timeline_events\n",
        "\n",
        "        # Heuristic checks detail\n",
        "        result[\"heuristicChecks\"] = {\n",
        "            \"visual_clip\": clip_score,\n",
        "            \"gemini_visual_artifacts\": vis_flag,\n",
        "            \"gemini_lipsync_issue\": lip_flag,\n",
        "            \"gemini_blink_abnormality\": blink_flag,\n",
        "            \"gibberish\": gibberish_score,\n",
        "            \"flow\": flow_result.get(\"score\", 0.0),\n",
        "        }\n",
        "\n",
        "        result[\"processing_time\"] = round(time.time() - start_time, 2)\n",
        "        print(f\"✅ Pipeline completed successfully in {result['processing_time']:.2f}s\")\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"Pipeline error for {video_basename}: {e}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "        result.update({\n",
        "            \"error\": error_msg,\n",
        "            \"trace\": traceback.format_exc(),\n",
        "            \"final_predicted_label\": \"ERROR_IN_PROCESSING\",\n",
        "            \"deepfake_confidence_overall\": 0.5,\n",
        "            \"label_confidence\": 0.5,\n",
        "            \"anomaly_tags_detected\": [\"PIPELINE_ERROR\"],\n",
        "            \"heuristicChecks\": {},\n",
        "            \"events\": [],\n",
        "            \"score_visual_clip\": 0.0,\n",
        "            \"flag_gemini_visual_artifact\": 0,\n",
        "            \"flag_gemini_lipsync_issue\": 0,\n",
        "            \"flag_gemini_abnormal_blinks\": 0,\n",
        "            \"processing_time\": round(time.time() - start_time, 2)\n",
        "        })\n",
        "\n",
        "    finally:\n",
        "        if temp_audio_path and os.path.exists(temp_audio_path):\n",
        "            try:\n",
        "                os.remove(temp_audio_path)\n",
        "            except OSError:\n",
        "                pass\n",
        "\n",
        "    return result\n",
        "\n",
        "def format_api_response(pipeline_result: Dict[str, Any], job_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Format pipeline result to match the API response structure from backend/app/schemas.py\n",
        "    \"\"\"\n",
        "    processing_time = pipeline_result.get(\"processing_time\", 0.0)\n",
        "\n",
        "    # Map internal anomaly tags to user-friendly descriptions\n",
        "    tag_mapping = {\n",
        "        \"VISUAL_CLIP_ANOMALY\": \"Visual Anomaly Detected\",\n",
        "        \"GEMINI_VISUAL_ARTIFACTS\": \"Visual Artifacts Detected\",\n",
        "        \"GEMINI_LIPSYNC_ISSUE\": \"Lip-sync Issue Detected\",\n",
        "        \"GEMINI_ABNORMAL_BLINKS\": \"Abnormal Blinking Pattern\",\n",
        "        \"gibberish_text\": \"Gibberish Text Detected\",\n",
        "        \"flow_spike\": \"Motion Flow Anomaly\",\n",
        "        \"PIPELINE_ERROR\": \"Processing Error\"\n",
        "    }\n",
        "\n",
        "    mapped_tags = [tag_mapping.get(tag, tag) for tag in pipeline_result.get(\"anomaly_tags_detected\", [])]\n",
        "\n",
        "    return {\n",
        "        \"job_id\": job_id,\n",
        "        \"status\": \"completed\",\n",
        "        \"result\": {\n",
        "            \"id\": pipeline_result.get(\"run_id\", job_id),\n",
        "            \"isReal\": pipeline_result.get(\"final_predicted_label\", \"ERROR_IN_PROCESSING\") == \"LIKELY_REAL\",\n",
        "            \"label\": pipeline_result.get(\"final_predicted_label\", \"ERROR_IN_PROCESSING\"),\n",
        "            \"confidenceScore\": pipeline_result.get(\"label_confidence\", 0.5),\n",
        "            \"processedAt\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"tags\": mapped_tags,\n",
        "            \"details\": {\n",
        "                \"visualScore\": pipeline_result.get(\"score_visual_clip\", 0.0),\n",
        "                \"processingTime\": processing_time,\n",
        "                \"videoLength\": pipeline_result.get(\"video_processed_duration_sec\", 0.0),\n",
        "                \"originalVideoLength\": pipeline_result.get(\"video_original_duration_sec\", 0.0),\n",
        "                \"pipelineVersion\": pipeline_result.get(\"pipeline_version\", \"unknown\"),\n",
        "                \"transcriptSnippet\": pipeline_result.get(\"transcript_snippet\", \"N/A\"),\n",
        "                \"geminiChecks\": {\n",
        "                    \"visualArtifacts\": bool(pipeline_result.get(\"flag_gemini_visual_artifact\", 0)),\n",
        "                    \"lipsyncIssue\": bool(pipeline_result.get(\"flag_gemini_lipsync_issue\", 0)),\n",
        "                    \"abnormalBlinks\": bool(pipeline_result.get(\"flag_gemini_abnormal_blinks\", 0))\n",
        "                },\n",
        "                \"heuristicChecks\": pipeline_result.get(\"heuristicChecks\", {}),\n",
        "                \"detectedLanguage\": pipeline_result.get(\"detected_language\", \"unknown\"),\n",
        "                \"error_message\": pipeline_result.get(\"error\"),\n",
        "                \"error_trace\": pipeline_result.get(\"trace\")\n",
        "            },\n",
        "            \"events\": pipeline_result.get(\"events\", [])\n",
        "        },\n",
        "        \"processing_time\": processing_time\n",
        "    }\n",
        "\n",
        "# Helper function to run async code from notebook\n",
        "def run_async(coro):\n",
        "    \"\"\"Run async code in notebook\"\"\"\n",
        "    try:\n",
        "        loop = asyncio.get_running_loop()\n",
        "        return loop.run_until_complete(coro)\n",
        "    except RuntimeError:\n",
        "        return asyncio.run(coro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sEPKpmximnAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fe7c94-1ea7-4d6d-b9b7-91ae91748a51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎬 DEEPFAKE DETECTION DEMO\n",
            "============================================================\n",
            "✅ Backend modules loaded successfully!\n",
            "\n",
            "📋 VIDEO UPLOAD INSTRUCTIONS:\n",
            "\n",
            "1. 📁 Click the 'Files' tab in the left sidebar of Colab\n",
            "2. 📤 Click 'Upload to session storage' \n",
            "3. 🎬 Select a video file (MP4, AVI, MOV, MKV, WebM)\n",
            "4. ✏️ Update the TEST_VIDEO_PATH variable below with your filename\n",
            "\n",
            "SAMPLE VIDEOS RECOMMENDED:\n",
            "• Real video: Genuine person speaking (news clips, interviews)\n",
            "• Fake video: Known deepfake content for comparison\n",
            "\n",
            "PROCESSING NOTES:\n",
            "• Videos processed for up to 30 seconds maximum\n",
            "• Works best with clear faces and speech\n",
            "• Non-English speech disables lip-sync but other checks remain active\n",
            "• Gemini API calls may take 30-120 seconds depending on content\n",
            "\n",
            "\n",
            "📂 Found 1 video file(s) in /content:\n",
            "   • fake_news.mp4 (1.2 MB)\n",
            "\n",
            "🎬 Ready to analyze! Update TEST_VIDEO_PATH above and run the cell below.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Demo & Testing Setup\n",
        "\n",
        "print(\"🎬 DEEPFAKE DETECTION DEMO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check if backend modules are loaded first\n",
        "missing_modules = []\n",
        "required_modules = ['flow', 'fusion', 'gemini', 'models', 'video']\n",
        "for mod in required_modules:\n",
        "    if mod not in globals():\n",
        "        missing_modules.append(mod)\n",
        "\n",
        "if missing_modules:\n",
        "    print(\"❌ SETUP INCOMPLETE!\")\n",
        "    print(\"Missing backend modules:\", ', '.join(missing_modules))\n",
        "    print(\"\"\"\n",
        "🚨 REQUIRED SETUP STEPS:\n",
        "\n",
        "1. 📄 Upload Backend Files:\n",
        "   • Go to Files tab → Upload to session storage\n",
        "   • Upload ALL 5 files: flow.py, fusion.py, gemini.py, models.py, video.py\n",
        "   • These files are from the backend/app/core/ directory\n",
        "\n",
        "2. 📤 Run Cell 2 again to load the modules\n",
        "\n",
        "3. 📹 Then upload your test video and run this demo\n",
        "\n",
        "⚠️  The backend files must be the actual files from the repository!\n",
        "\"\"\")\n",
        "else:\n",
        "    print(\"✅ Backend modules loaded successfully!\")\n",
        "    print(\"\"\"\n",
        "📋 VIDEO UPLOAD INSTRUCTIONS:\n",
        "\n",
        "1. 📁 Click the 'Files' tab in the left sidebar of Colab\n",
        "2. 📤 Click 'Upload to session storage'\n",
        "3. 🎬 Select a video file (MP4, AVI, MOV, MKV, WebM)\n",
        "4. ✏️ Update the TEST_VIDEO_PATH variable below with your filename\n",
        "\n",
        "SAMPLE VIDEOS RECOMMENDED:\n",
        "• Real video: Genuine person speaking (news clips, interviews)\n",
        "• Fake video: Known deepfake content for comparison\n",
        "\n",
        "PROCESSING NOTES:\n",
        "• Videos processed for up to 30 seconds maximum\n",
        "• Works best with clear faces and speech\n",
        "• Non-English speech disables lip-sync but other checks remain active\n",
        "• Gemini API calls may take 30-120 seconds depending on content\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Configure your test video path here\n",
        "TEST_VIDEO_PATH = \"/content/fake_news.mp4\"  # ⚠️ CHANGE THIS TO YOUR UPLOADED VIDEO\n",
        "\n",
        "# Demo helper functions\n",
        "def find_uploaded_videos():\n",
        "    \"\"\"Find video files in /content directory\"\"\"\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.webm', '.m4v']\n",
        "    found_videos = []\n",
        "\n",
        "    try:\n",
        "        for file in os.listdir('/content'):\n",
        "            if any(file.lower().endswith(ext) for ext in video_extensions):\n",
        "                file_path = f\"/content/{file}\"\n",
        "                if os.path.getsize(file_path) > 1000:  # At least 1KB\n",
        "                    size_mb = os.path.getsize(file_path) / (1024*1024)\n",
        "                    found_videos.append((file, size_mb))\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    return found_videos\n",
        "\n",
        "def check_video_file(video_path):\n",
        "    \"\"\"Check if video file exists and is valid\"\"\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"❌ Video not found: {video_path}\")\n",
        "\n",
        "        # Try to find available videos\n",
        "        found_videos = find_uploaded_videos()\n",
        "        if found_videos:\n",
        "            print(f\"\\n💡 Found {len(found_videos)} video file(s) in /content:\")\n",
        "            for video, size_mb in found_videos:\n",
        "                print(f\"   • {video} ({size_mb:.1f} MB)\")\n",
        "            print(f\"\\n📝 Try setting: TEST_VIDEO_PATH = \\\"/content/{found_videos[0][0]}\\\"\")\n",
        "        else:\n",
        "            print(\"\\n🔧 SETUP REQUIRED:\")\n",
        "            print(\"1. Upload a video file to Colab using the Files tab\")\n",
        "            print(\"2. Update TEST_VIDEO_PATH variable above\")\n",
        "            print(\"3. Re-run this cell\")\n",
        "        return False\n",
        "\n",
        "    size_mb = os.path.getsize(video_path) / (1024*1024)\n",
        "    if size_mb < 0.001:  # Less than 1KB\n",
        "        print(f\"❌ Video file too small: {size_mb:.3f} MB\")\n",
        "        return False\n",
        "\n",
        "    print(f\"✅ Video found: {os.path.basename(video_path)} ({size_mb:.1f} MB)\")\n",
        "    return True\n",
        "\n",
        "async def run_demo():\n",
        "    \"\"\"Run the detection pipeline and display results\"\"\"\n",
        "    if not check_video_file(TEST_VIDEO_PATH):\n",
        "        return\n",
        "\n",
        "    print(f\"\\n🚀 Starting deepfake detection pipeline...\")\n",
        "    print(f\"📁 Input: {os.path.basename(TEST_VIDEO_PATH)}\")\n",
        "\n",
        "    # Generate demo job ID\n",
        "    demo_job_id = f\"demo_{uuid.uuid4().hex[:8]}\"\n",
        "\n",
        "    # Run the pipeline\n",
        "    pipeline_result = await run_detection_pipeline(TEST_VIDEO_PATH, demo_job_id)\n",
        "\n",
        "    # Format as API response\n",
        "    api_response = format_api_response(pipeline_result, demo_job_id)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"🎯 DETECTION RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    result = api_response[\"result\"]\n",
        "\n",
        "    # Main verdict\n",
        "    print(f\"🏷️  VERDICT: {result['label']}\")\n",
        "    print(f\"🎭 Is Real: {'✅ YES' if result['isReal'] else '❌ NO'}\")\n",
        "    print(f\"📊 Confidence: {result['confidenceScore']:.1%}\")\n",
        "    print(f\"⏱️  Processing Time: {api_response['processing_time']:.1f}s\")\n",
        "\n",
        "    # Tags\n",
        "    if result['tags']:\n",
        "        print(f\"🚩 Detected Issues: {', '.join(result['tags'])}\")\n",
        "    else:\n",
        "        print(\"🚩 Detected Issues: None\")\n",
        "\n",
        "    # Technical details\n",
        "    details = result['details']\n",
        "    print(f\"\\n📹 Video Info:\")\n",
        "    print(f\"   • Length: {details['videoLength']:.1f}s (original: {details['originalVideoLength']:.1f}s)\")\n",
        "    print(f\"   • Language: {details.get('detectedLanguage', 'unknown')}\")\n",
        "    print(f\"   • Transcript: {details['transcriptSnippet']}\")\n",
        "\n",
        "    print(f\"\\n🔍 Component Scores:\")\n",
        "    heuristics = details['heuristicChecks']\n",
        "    for component, score in heuristics.items():\n",
        "        if isinstance(score, (int, float)):\n",
        "            print(f\"   • {component.replace('_', ' ').title()}: {score:.3f}\")\n",
        "\n",
        "    print(f\"\\n🔮 Gemini Checks:\")\n",
        "    gemini = details['geminiChecks']\n",
        "    for check, detected in gemini.items():\n",
        "        status = \"🔴 DETECTED\" if detected else \"🟢 CLEAN\"\n",
        "        print(f\"   • {check.replace('_', ' ').title()}: {status}\")\n",
        "\n",
        "    # Timeline events\n",
        "    events = result.get('events', [])\n",
        "    if events:\n",
        "        print(f\"\\n⏰ Timeline Events ({len(events)} total):\")\n",
        "        for i, event in enumerate(events[:5]):  # Show first 5 events\n",
        "            module = event.get('module', 'unknown')\n",
        "            event_type = event.get('event', 'unknown')\n",
        "            ts = event.get('ts', 0)\n",
        "            print(f\"   {i+1}. [{module}] {event_type} @ {ts:.1f}s\")\n",
        "        if len(events) > 5:\n",
        "            print(f\"   ... and {len(events) - 5} more events\")\n",
        "    else:\n",
        "        print(\"\\n⏰ Timeline Events: None detected\")\n",
        "\n",
        "    # Error handling\n",
        "    if details.get('error_message'):\n",
        "        print(f\"\\n⚠️  Pipeline Error: {details['error_message']}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"📄 Raw API Response (JSON):\")\n",
        "    print(\"=\"*80)\n",
        "    print(json.dumps(api_response, indent=2))\n",
        "\n",
        "    return api_response\n",
        "\n",
        "# Synchronous wrapper for easy execution\n",
        "def run_demo_sync():\n",
        "    \"\"\"Synchronous wrapper for the demo\"\"\"\n",
        "    return run_async(run_demo())\n",
        "\n",
        "# Check for uploaded videos on cell execution\n",
        "found_videos = find_uploaded_videos()\n",
        "if found_videos:\n",
        "    print(f\"📂 Found {len(found_videos)} video file(s) in /content:\")\n",
        "    for video, size_mb in found_videos:\n",
        "        print(f\"   • {video} ({size_mb:.1f} MB)\")\n",
        "\n",
        "print(\"\\n🎬 Ready to analyze! Update TEST_VIDEO_PATH above and run the cell below.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpAeWa1BmrdN",
        "outputId": "727359b9-d067-4c3c-ed12-afda2c1e2e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Video found: fake_news.mp4 (1.2 MB)\n",
            "\n",
            "🚀 Starting deepfake detection pipeline...\n",
            "📁 Input: fake_news.mp4\n",
            "🔍 Processing: fake_news.mp4\n",
            "📹 Step 1: Sampling video content...\n",
            "🔧 Video function not found, trying direct import...\n",
            "✅ Using directly imported video module\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FFmpeg extracted 191 frames (target max: 192).\n",
            "CLIP Debug: Processing 191 frames on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎨 Step 2: Calculating CLIP visual score...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CLIP Debug: Final score: 0.503 (scaled: 0.011)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎤 Step 3: Transcribing audio with Whisper...\n",
            "🔮 Step 4: Running Gemini inspections...\n",
            "🔍 Debug: Gemini model type: <class 'google.generativeai.generative_models.GenerativeModel'>\n",
            "🔍 Debug: Number of frames: 191\n",
            "🔍 Debug: Video duration: 24.03s\n",
            "🔍 Debug: Lipsync enabled: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:gemini:[safe_generate_content] Unexpected exception: ReadTimeout: HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gemini.py\", line 72, in safe_generate_content\n",
            "    return await model.generate_content_async(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 309, in generate_content_async\n",
            "    response = await self._async_client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py\", line 395, in generate_content\n",
            "    response = await rpc(\n",
            "               ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 231, in retry_wrapped_func\n",
            "    return await retry_target(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 163, in retry_target\n",
            "    next_sleep = _retry_error_helper(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 158, in retry_target\n",
            "    return await target()\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers_async.py\", line 165, in error_remapped_callable\n",
            "    call = callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 835, in __call__\n",
            "    response = getattr(self._session, method)(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\", line 540, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "GEMINI_ERROR (gemini_check_visual_artifacts): HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gemini.py\", line 288, in gemini_check_visual_artifacts\n",
            "    resp = await safe_generate_content(model, parts)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gemini.py\", line 72, in safe_generate_content\n",
            "    return await model.generate_content_async(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 309, in generate_content_async\n",
            "    response = await self._async_client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py\", line 395, in generate_content\n",
            "    response = await rpc(\n",
            "               ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 231, in retry_wrapped_func\n",
            "    return await retry_target(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 163, in retry_target\n",
            "    next_sleep = _retry_error_helper(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 158, in retry_target\n",
            "    return await target()\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers_async.py\", line 165, in error_remapped_callable\n",
            "    call = callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 835, in __call__\n",
            "    response = getattr(self._session, method)(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\", line 540, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "ERROR:gemini:[safe_generate_content] Unexpected exception: ReadTimeout: HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gemini.py\", line 72, in safe_generate_content\n",
            "    return await model.generate_content_async(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 309, in generate_content_async\n",
            "    response = await self._async_client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py\", line 395, in generate_content\n",
            "    response = await rpc(\n",
            "               ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 231, in retry_wrapped_func\n",
            "    return await retry_target(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 163, in retry_target\n",
            "    next_sleep = _retry_error_helper(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 158, in retry_target\n",
            "    return await target()\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers_async.py\", line 165, in error_remapped_callable\n",
            "    call = callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 835, in __call__\n",
            "    response = getattr(self._session, method)(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\", line 540, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "GEMINI_ERROR (gemini_check_abnormal_blinks): HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gemini.py\", line 331, in gemini_check_abnormal_blinks\n",
            "    resp = await safe_generate_content(model, parts)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gemini.py\", line 72, in safe_generate_content\n",
            "    return await model.generate_content_async(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 309, in generate_content_async\n",
            "    response = await self._async_client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py\", line 395, in generate_content\n",
            "    response = await rpc(\n",
            "               ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 231, in retry_wrapped_func\n",
            "    return await retry_target(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 163, in retry_target\n",
            "    next_sleep = _retry_error_helper(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 158, in retry_target\n",
            "    return await target()\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers_async.py\", line 165, in error_remapped_callable\n",
            "    call = callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 835, in __call__\n",
            "    response = getattr(self._session, method)(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\", line 540, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "WARNING:gemini:[gemini_detect_gibberish] No model or no input. Skipping.\n",
            "ERROR:gemini:[safe_generate_content] Unexpected exception: ReadTimeout: HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gemini.py\", line 72, in safe_generate_content\n",
            "    return await model.generate_content_async(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 309, in generate_content_async\n",
            "    response = await self._async_client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py\", line 395, in generate_content\n",
            "    response = await rpc(\n",
            "               ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 231, in retry_wrapped_func\n",
            "    return await retry_target(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 163, in retry_target\n",
            "    next_sleep = _retry_error_helper(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 158, in retry_target\n",
            "    return await target()\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers_async.py\", line 165, in error_remapped_callable\n",
            "    call = callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 835, in __call__\n",
            "    response = getattr(self._session, method)(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\", line 540, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "GEMINI_ERROR (gemini_check_lipsync): HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gemini.py\", line 425, in gemini_check_lipsync\n",
            "    resp = await safe_generate_content(model, parts)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gemini.py\", line 72, in safe_generate_content\n",
            "    return await model.generate_content_async(content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 309, in generate_content_async\n",
            "    response = await self._async_client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py\", line 395, in generate_content\n",
            "    response = await rpc(\n",
            "               ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 231, in retry_wrapped_func\n",
            "    return await retry_target(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 163, in retry_target\n",
            "    next_sleep = _retry_error_helper(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary_async.py\", line 158, in retry_target\n",
            "    return await target()\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers_async.py\", line 165, in error_remapped_callable\n",
            "    call = callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 835, in __call__\n",
            "    response = getattr(self._session, method)(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\", line 540, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=35211): Read timed out. (read timeout=60.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Debug: Gemini results - vis:0, lip:1, blink:0, gibberish:0.000\n",
            "🔬 Step 5: Running heuristic detectors...\n",
            "⚖️ Step 6: Fusing detection scores...\n",
            "✅ Pipeline completed successfully in 216.40s\n",
            "\n",
            "================================================================================\n",
            "🎯 DETECTION RESULTS\n",
            "================================================================================\n",
            "🏷️  VERDICT: LIKELY_REAL\n",
            "🎭 Is Real: ✅ YES\n",
            "📊 Confidence: 72.7%\n",
            "⏱️  Processing Time: 216.4s\n",
            "🚩 Detected Issues: Lip-sync Issue Detected, Motion Flow Anomaly\n",
            "\n",
            "📹 Video Info:\n",
            "   • Length: 24.0s (original: 24.0s)\n",
            "   • Language: en\n",
            "   • Transcript: Angela Carter here live from the Cedar Grove flood disaster. Just kidding. I'm not real This is Dana Brooks reporting live from Oceanview Beach Just k...\n",
            "\n",
            "🔍 Component Scores:\n",
            "   • Visual Clip: 0.503\n",
            "   • Gemini Visual Artifacts: 0.000\n",
            "   • Gemini Lipsync Issue: 1.000\n",
            "   • Gemini Blink Abnormality: 0.000\n",
            "   • Gibberish: 0.000\n",
            "   • Flow: 0.100\n",
            "\n",
            "🔮 Gemini Checks:\n",
            "   • Visualartifacts: 🟢 CLEAN\n",
            "   • Lipsyncissue: 🔴 DETECTED\n",
            "   • Abnormalblinks: 🟢 CLEAN\n",
            "\n",
            "⏰ Timeline Events (5 total):\n",
            "   1. [flow] flow_spike @ 5.4s\n",
            "   2. [flow] flow_spike @ 6.6s\n",
            "   3. [flow] flow_spike @ 7.6s\n",
            "   4. [flow] flow_spike @ 15.8s\n",
            "   5. [lip_sync] check_failed @ 0.0s\n",
            "\n",
            "================================================================================\n",
            "📄 Raw API Response (JSON):\n",
            "================================================================================\n",
            "{\n",
            "  \"job_id\": \"demo_761ea923\",\n",
            "  \"status\": \"completed\",\n",
            "  \"result\": {\n",
            "    \"id\": \"fake_news_demo_7\",\n",
            "    \"isReal\": true,\n",
            "    \"label\": \"LIKELY_REAL\",\n",
            "    \"confidenceScore\": 0.727,\n",
            "    \"processedAt\": \"2025-06-17T02:44:50.322939Z\",\n",
            "    \"tags\": [\n",
            "      \"Lip-sync Issue Detected\",\n",
            "      \"Motion Flow Anomaly\"\n",
            "    ],\n",
            "    \"details\": {\n",
            "      \"visualScore\": 0.503,\n",
            "      \"processingTime\": 216.4,\n",
            "      \"videoLength\": 24.03,\n",
            "      \"originalVideoLength\": 24.03,\n",
            "      \"pipelineVersion\": \"notebook_backend_integration_v1\",\n",
            "      \"transcriptSnippet\": \"Angela Carter here live from the Cedar Grove flood disaster. Just kidding. I'm not real This is Dana Brooks reporting live from Oceanview Beach Just k...\",\n",
            "      \"geminiChecks\": {\n",
            "        \"visualArtifacts\": false,\n",
            "        \"lipsyncIssue\": true,\n",
            "        \"abnormalBlinks\": false\n",
            "      },\n",
            "      \"heuristicChecks\": {\n",
            "        \"visual_clip\": 0.5028114842096328,\n",
            "        \"gemini_visual_artifacts\": 0,\n",
            "        \"gemini_lipsync_issue\": 1,\n",
            "        \"gemini_blink_abnormality\": 0,\n",
            "        \"gibberish\": 0.0,\n",
            "        \"flow\": 0.1\n",
            "      },\n",
            "      \"detectedLanguage\": \"en\",\n",
            "      \"error_message\": null,\n",
            "      \"error_trace\": null\n",
            "    },\n",
            "    \"events\": [\n",
            "      {\n",
            "        \"module\": \"flow\",\n",
            "        \"event\": \"flow_spike\",\n",
            "        \"ts\": 5.44,\n",
            "        \"dur\": 0.0,\n",
            "        \"meta\": {\n",
            "          \"z\": 3.0,\n",
            "          \"ssim\": 0.555\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"module\": \"flow\",\n",
            "        \"event\": \"flow_spike\",\n",
            "        \"ts\": 6.56,\n",
            "        \"dur\": 0.0,\n",
            "        \"meta\": {\n",
            "          \"z\": 2.31,\n",
            "          \"ssim\": 0.703\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"module\": \"flow\",\n",
            "        \"event\": \"flow_spike\",\n",
            "        \"ts\": 7.56,\n",
            "        \"dur\": 0.0,\n",
            "        \"meta\": {\n",
            "          \"z\": 2.36,\n",
            "          \"ssim\": 0.665\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"module\": \"flow\",\n",
            "        \"event\": \"flow_spike\",\n",
            "        \"ts\": 15.81,\n",
            "        \"dur\": 0.0,\n",
            "        \"meta\": {\n",
            "          \"z\": 5.84,\n",
            "          \"ssim\": 0.436\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"module\": \"lip_sync\",\n",
            "        \"event\": \"check_failed\",\n",
            "        \"ts\": 0.0,\n",
            "        \"dur\": 0.0,\n",
            "        \"meta\": {\n",
            "          \"reason\": \"An exception occurred during the check: ReadTimeout\"\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"processing_time\": 216.4\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'job_id': 'demo_761ea923',\n",
              " 'status': 'completed',\n",
              " 'result': {'id': 'fake_news_demo_7',\n",
              "  'isReal': True,\n",
              "  'label': 'LIKELY_REAL',\n",
              "  'confidenceScore': 0.727,\n",
              "  'processedAt': '2025-06-17T02:44:50.322939Z',\n",
              "  'tags': ['Lip-sync Issue Detected', 'Motion Flow Anomaly'],\n",
              "  'details': {'visualScore': 0.503,\n",
              "   'processingTime': 216.4,\n",
              "   'videoLength': 24.03,\n",
              "   'originalVideoLength': 24.03,\n",
              "   'pipelineVersion': 'notebook_backend_integration_v1',\n",
              "   'transcriptSnippet': \"Angela Carter here live from the Cedar Grove flood disaster. Just kidding. I'm not real This is Dana Brooks reporting live from Oceanview Beach Just k...\",\n",
              "   'geminiChecks': {'visualArtifacts': False,\n",
              "    'lipsyncIssue': True,\n",
              "    'abnormalBlinks': False},\n",
              "   'heuristicChecks': {'visual_clip': 0.5028114842096328,\n",
              "    'gemini_visual_artifacts': 0,\n",
              "    'gemini_lipsync_issue': 1,\n",
              "    'gemini_blink_abnormality': 0,\n",
              "    'gibberish': 0.0,\n",
              "    'flow': 0.1},\n",
              "   'detectedLanguage': 'en',\n",
              "   'error_message': None,\n",
              "   'error_trace': None},\n",
              "  'events': [{'module': 'flow',\n",
              "    'event': 'flow_spike',\n",
              "    'ts': 5.44,\n",
              "    'dur': 0.0,\n",
              "    'meta': {'z': 3.0, 'ssim': 0.555}},\n",
              "   {'module': 'flow',\n",
              "    'event': 'flow_spike',\n",
              "    'ts': 6.56,\n",
              "    'dur': 0.0,\n",
              "    'meta': {'z': 2.31, 'ssim': 0.703}},\n",
              "   {'module': 'flow',\n",
              "    'event': 'flow_spike',\n",
              "    'ts': 7.56,\n",
              "    'dur': 0.0,\n",
              "    'meta': {'z': 2.36, 'ssim': 0.665}},\n",
              "   {'module': 'flow',\n",
              "    'event': 'flow_spike',\n",
              "    'ts': 15.81,\n",
              "    'dur': 0.0,\n",
              "    'meta': {'z': 5.84, 'ssim': 0.436}},\n",
              "   {'module': 'lip_sync',\n",
              "    'event': 'check_failed',\n",
              "    'ts': 0.0,\n",
              "    'dur': 0.0,\n",
              "    'meta': {'reason': 'An exception occurred during the check: ReadTimeout'}}]},\n",
              " 'processing_time': 216.4}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Cell 5: Execute Detection Demo\n",
        "\n",
        "# 🎬 EXECUTE DETECTION\n",
        "# Run this cell after uploading your video and updating TEST_VIDEO_PATH above\n",
        "\n",
        "run_demo_sync()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIEkWZxocewS"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNjw3ft5qPn8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}